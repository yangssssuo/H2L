Traceback (most recent call last):
  File "unet_attn.py", line 19, in <module>
    data_set = FreqDataset(mode='IR',minmax=False)
  File "/home/yanggk/H2L/Data_Loader.py", line 44, in __init__
    L_wide_IR = self.make_noise(L_wide_IR)
  File "/home/yanggk/H2L/Data_Loader.py", line 113, in make_noise
    item[i][j] += noise[i][j]
KeyboardInterrupt
epoch: 0 train_loss: 94.88113403320312 val_loss: 68.42756652832031, save best model
epoch: 1 train_loss: 69.34756469726562 val_loss: 60.930870056152344, save best model
epoch: 2 train_loss: 61.714073181152344 val_loss: 57.25853729248047, save best model
epoch: 3 train_loss: 56.98238754272461 val_loss: 57.208866119384766, save best model
epoch: 4 train_loss: 52.96450424194336 val_loss: 55.5378532409668, save best model
epoch: 5 train_loss: 49.842979431152344 val_loss: 56.298912048339844, 
epoch: 6 train_loss: 46.67877197265625 val_loss: 55.04935836791992, save best model
epoch: 7 train_loss: 44.50905990600586 val_loss: 53.24303436279297, save best model
epoch: 8 train_loss: 42.91450500488281 val_loss: 53.669185638427734, 
epoch: 9 train_loss: 39.73169708251953 val_loss: 55.29196548461914, 
epoch: 10 train_loss: 37.31501770019531 val_loss: 55.154510498046875, 
epoch: 11 train_loss: 35.647254943847656 val_loss: 54.76735305786133, 
epoch: 12 train_loss: 33.44779968261719 val_loss: 57.246273040771484, 
epoch: 13 train_loss: 30.807373046875 val_loss: 55.493003845214844, 
epoch: 14 train_loss: 28.204998016357422 val_loss: 54.13493347167969, 
epoch: 15 train_loss: 26.405113220214844 val_loss: 54.991390228271484, 
epoch: 16 train_loss: 24.242839813232422 val_loss: 57.65479278564453, 
epoch: 17 train_loss: 22.601030349731445 val_loss: 58.03706359863281, 
epoch: 18 train_loss: 21.747438430786133 val_loss: 59.38332748413086, 
epoch: 19 train_loss: 21.073226928710938 val_loss: 57.615211486816406, 
epoch: 20 train_loss: 20.27562141418457 val_loss: 59.07139587402344, 
epoch: 21 train_loss: 19.361900329589844 val_loss: 55.610347747802734, 
epoch: 22 train_loss: 17.58320426940918 val_loss: 57.48297882080078, 
epoch: 23 train_loss: 16.55656623840332 val_loss: 55.79298400878906, 
epoch: 24 train_loss: 16.150367736816406 val_loss: 55.43303680419922, 
epoch: 25 train_loss: 15.614492416381836 val_loss: 55.03102111816406, 
epoch: 26 train_loss: 14.845440864562988 val_loss: 55.97282409667969, 
epoch: 27 train_loss: 14.099106788635254 val_loss: 58.979042053222656, 
epoch: 28 train_loss: 13.462906837463379 val_loss: 57.91642379760742, 
epoch: 29 train_loss: 13.052857398986816 val_loss: 57.19815444946289, 
epoch: 30 train_loss: 12.471485137939453 val_loss: 56.70911407470703, 
epoch: 31 train_loss: 12.069992065429688 val_loss: 55.23161697387695, 
epoch: 32 train_loss: 11.81299114227295 val_loss: 57.07601547241211, 
epoch: 33 train_loss: 11.725021362304688 val_loss: 56.42976379394531, 
epoch: 34 train_loss: 11.234965324401855 val_loss: 56.68510055541992, 
epoch: 35 train_loss: 10.930696487426758 val_loss: 55.524330139160156, 
epoch: 36 train_loss: 10.548047065734863 val_loss: 56.12184143066406, 
epoch: 37 train_loss: 10.273062705993652 val_loss: 57.36639404296875, 
epoch: 38 train_loss: 10.238022804260254 val_loss: 55.487300872802734, 
epoch: 39 train_loss: 10.028834342956543 val_loss: 57.61259078979492, 
epoch: 40 train_loss: 9.829219818115234 val_loss: 55.16550064086914, 
epoch: 41 train_loss: 9.602212905883789 val_loss: 54.10575866699219, 
epoch: 42 train_loss: 9.281217575073242 val_loss: 54.55764389038086, 
epoch: 43 train_loss: 8.793803215026855 val_loss: 55.56772994995117, 
epoch: 44 train_loss: 8.691755294799805 val_loss: 55.96882247924805, 
epoch: 45 train_loss: 8.610077857971191 val_loss: 54.3482551574707, 
epoch: 46 train_loss: 8.814764022827148 val_loss: 54.295291900634766, 
epoch: 47 train_loss: 8.809388160705566 val_loss: 55.88435363769531, 
epoch: 48 train_loss: 8.965160369873047 val_loss: 54.89591598510742, 
epoch: 49 train_loss: 8.992857933044434 val_loss: 55.3825569152832, 
epoch: 50 train_loss: 9.083057403564453 val_loss: 56.9387321472168, 
epoch: 51 train_loss: 9.365630149841309 val_loss: 56.79668426513672, 
epoch: 52 train_loss: 9.114373207092285 val_loss: 56.30268859863281, 
epoch: 53 train_loss: 8.851066589355469 val_loss: 55.117088317871094, 
epoch: 54 train_loss: 8.709598541259766 val_loss: 56.45832443237305, 
epoch: 55 train_loss: 8.744296073913574 val_loss: 56.49068069458008, 
epoch: 56 train_loss: 8.631793975830078 val_loss: 56.54379653930664, 
epoch: 57 train_loss: 8.489530563354492 val_loss: 55.75817108154297, 
Epoch 00059: reducing learning rate of group 0 to 5.0000e-04.
epoch: 58 train_loss: 8.363000869750977 val_loss: 56.291446685791016, 
epoch: 59 train_loss: 7.589613437652588 val_loss: 54.5802116394043, 
epoch: 60 train_loss: 6.167628288269043 val_loss: 54.369815826416016, 
epoch: 61 train_loss: 5.59250020980835 val_loss: 53.95394515991211, 
epoch: 62 train_loss: 5.37345552444458 val_loss: 53.67020797729492, 
epoch: 63 train_loss: 5.315845966339111 val_loss: 53.34040069580078, 
epoch: 64 train_loss: 5.220149517059326 val_loss: 53.33938217163086, 
epoch: 65 train_loss: 5.187485218048096 val_loss: 53.35850524902344, 
epoch: 66 train_loss: 5.080211162567139 val_loss: 53.074745178222656, save best model
epoch: 67 train_loss: 4.997988700866699 val_loss: 53.037288665771484, save best model
epoch: 68 train_loss: 4.934554576873779 val_loss: 52.86872863769531, save best model
epoch: 69 train_loss: 4.88715934753418 val_loss: 53.01820373535156, 
epoch: 70 train_loss: 4.865832805633545 val_loss: 52.96912384033203, 
epoch: 71 train_loss: 4.842334270477295 val_loss: 52.949100494384766, 
epoch: 72 train_loss: 4.81028413772583 val_loss: 52.81493377685547, save best model
epoch: 73 train_loss: 4.819454193115234 val_loss: 52.83322525024414, 
epoch: 74 train_loss: 4.823242664337158 val_loss: 52.63089370727539, save best model
epoch: 75 train_loss: 4.811704635620117 val_loss: 52.454689025878906, save best model
epoch: 76 train_loss: 4.804673671722412 val_loss: 52.58976745605469, 
epoch: 77 train_loss: 4.8126726150512695 val_loss: 52.530433654785156, 
epoch: 78 train_loss: 4.8773908615112305 val_loss: 52.85052490234375, 
epoch: 79 train_loss: 4.922068119049072 val_loss: 53.14075469970703, 
epoch: 80 train_loss: 5.029586315155029 val_loss: 53.559486389160156, 
epoch: 81 train_loss: 5.08640718460083 val_loss: 53.41761016845703, 
epoch: 82 train_loss: 5.136545181274414 val_loss: 53.45891571044922, 
epoch: 83 train_loss: 5.17769718170166 val_loss: 53.015010833740234, 
epoch: 84 train_loss: 5.186995506286621 val_loss: 52.42398452758789, save best model
epoch: 85 train_loss: 5.170929431915283 val_loss: 52.34975814819336, save best model
epoch: 86 train_loss: 5.130664348602295 val_loss: 52.082210540771484, save best model
epoch: 87 train_loss: 5.0601487159729 val_loss: 52.40314865112305, 
epoch: 88 train_loss: 5.026774883270264 val_loss: 52.82537841796875, 
epoch: 89 train_loss: 5.041847229003906 val_loss: 52.67399597167969, 
epoch: 90 train_loss: 5.003929615020752 val_loss: 52.85472106933594, 
epoch: 91 train_loss: 5.022760391235352 val_loss: 53.03795623779297, 
epoch: 92 train_loss: 5.026758193969727 val_loss: 53.28356170654297, 
epoch: 93 train_loss: 5.031118392944336 val_loss: 53.934043884277344, 
epoch: 94 train_loss: 5.050387382507324 val_loss: 53.961483001708984, 
epoch: 95 train_loss: 5.043378829956055 val_loss: 54.227020263671875, 
epoch: 96 train_loss: 4.960669040679932 val_loss: 53.838600158691406, 
epoch: 97 train_loss: 4.913344860076904 val_loss: 53.30514907836914, 
epoch: 98 train_loss: 4.87984561920166 val_loss: 52.83610534667969, 
epoch: 99 train_loss: 4.787607192993164 val_loss: 52.61110305786133, 
epoch: 100 train_loss: 4.8269124031066895 val_loss: 52.475746154785156, 
epoch: 101 train_loss: 4.991534233093262 val_loss: 51.82999038696289, save best model
epoch: 102 train_loss: 4.92006254196167 val_loss: 52.681663513183594, 
epoch: 103 train_loss: 4.995456218719482 val_loss: 53.15317916870117, 
epoch: 104 train_loss: 5.115921974182129 val_loss: 53.55601501464844, 
epoch: 105 train_loss: 5.255937576293945 val_loss: 53.67738723754883, 
epoch: 106 train_loss: 5.338741302490234 val_loss: 53.30947494506836, 
epoch: 107 train_loss: 5.368143558502197 val_loss: 53.63324737548828, 
epoch: 108 train_loss: 5.340971946716309 val_loss: 54.29494857788086, 
epoch: 109 train_loss: 5.281466484069824 val_loss: 55.451629638671875, 
epoch: 110 train_loss: 5.298817157745361 val_loss: 54.7966423034668, 
epoch: 111 train_loss: 5.39127254486084 val_loss: 53.47557067871094, 
epoch: 112 train_loss: 5.381806373596191 val_loss: 52.2839241027832, 
epoch: 113 train_loss: 5.289263725280762 val_loss: 52.6480598449707, 
epoch: 114 train_loss: 5.270434856414795 val_loss: 53.90628433227539, 
epoch: 115 train_loss: 5.260287284851074 val_loss: 53.92619323730469, 
epoch: 116 train_loss: 5.184395790100098 val_loss: 53.24449157714844, 
epoch: 117 train_loss: 5.234168529510498 val_loss: 52.86676788330078, 
epoch: 118 train_loss: 5.282703876495361 val_loss: 53.54341506958008, 
epoch: 119 train_loss: 5.3223042488098145 val_loss: 54.622467041015625, 
epoch: 120 train_loss: 5.210238456726074 val_loss: 54.344295501708984, 
epoch: 121 train_loss: 5.176594257354736 val_loss: 53.296661376953125, 
epoch: 122 train_loss: 5.376720905303955 val_loss: 51.47876739501953, save best model
epoch: 123 train_loss: 5.405265808105469 val_loss: 51.671661376953125, 
epoch: 124 train_loss: 5.598355293273926 val_loss: 53.0137939453125, 
epoch: 125 train_loss: 5.76024055480957 val_loss: 54.41648483276367, 
epoch: 126 train_loss: 5.5098443031311035 val_loss: 53.21866226196289, 
epoch: 127 train_loss: 5.558778762817383 val_loss: 52.53758239746094, 
epoch: 128 train_loss: 5.349930286407471 val_loss: 53.56047058105469, 
epoch: 129 train_loss: 5.250412464141846 val_loss: 53.65135192871094, 
epoch: 130 train_loss: 5.095295429229736 val_loss: 52.62903594970703, 
epoch: 131 train_loss: 5.105535507202148 val_loss: 52.81123352050781, 
epoch: 132 train_loss: 5.06706428527832 val_loss: 53.069976806640625, 
epoch: 133 train_loss: 4.927694320678711 val_loss: 53.40742111206055, 
epoch: 134 train_loss: 4.912651062011719 val_loss: 53.26516342163086, 
epoch: 135 train_loss: 4.926738262176514 val_loss: 53.32179641723633, 
epoch: 136 train_loss: 4.907102584838867 val_loss: 52.8923225402832, 
epoch: 137 train_loss: 4.848152160644531 val_loss: 53.261199951171875, 
epoch: 138 train_loss: 4.761754989624023 val_loss: 53.65167236328125, 
epoch: 139 train_loss: 4.724187850952148 val_loss: 54.1444206237793, 
epoch: 140 train_loss: 4.717898845672607 val_loss: 54.141117095947266, 
epoch: 141 train_loss: 4.728809833526611 val_loss: 53.64684295654297, 
epoch: 142 train_loss: 4.776723384857178 val_loss: 52.57038116455078, 
epoch: 143 train_loss: 4.77560567855835 val_loss: 51.973690032958984, 
epoch: 144 train_loss: 4.8365092277526855 val_loss: 52.43623352050781, 
epoch: 145 train_loss: 4.894974231719971 val_loss: 52.90605926513672, 
epoch: 146 train_loss: 4.969457626342773 val_loss: 52.468807220458984, 
epoch: 147 train_loss: 4.976800918579102 val_loss: 51.95429229736328, 
epoch: 148 train_loss: 5.033277988433838 val_loss: 52.962947845458984, 
epoch: 149 train_loss: 5.055599689483643 val_loss: 53.84381866455078, 
epoch: 150 train_loss: 4.980710983276367 val_loss: 53.10455322265625, 
epoch: 151 train_loss: 4.89681339263916 val_loss: 51.90725326538086, 
epoch: 152 train_loss: 4.747203350067139 val_loss: 52.62255096435547, 
epoch: 153 train_loss: 4.756800651550293 val_loss: 53.261329650878906, 
epoch: 154 train_loss: 4.758716106414795 val_loss: 53.16325759887695, 
epoch: 155 train_loss: 4.664723873138428 val_loss: 52.34697723388672, 
epoch: 156 train_loss: 4.649024963378906 val_loss: 52.85099411010742, 
epoch: 157 train_loss: 4.640768051147461 val_loss: 53.800819396972656, 
epoch: 158 train_loss: 4.632914066314697 val_loss: 54.02948760986328, 
epoch: 159 train_loss: 4.622261047363281 val_loss: 53.7002067565918, 
epoch: 160 train_loss: 4.556331634521484 val_loss: 53.51146697998047, 
epoch: 161 train_loss: 4.613649845123291 val_loss: 53.062889099121094, 
epoch: 162 train_loss: 4.670047283172607 val_loss: 53.586036682128906, 
epoch: 163 train_loss: 4.713947296142578 val_loss: 53.737186431884766, 
epoch: 164 train_loss: 4.69354772567749 val_loss: 52.68231201171875, 
epoch: 165 train_loss: 4.606525897979736 val_loss: 52.35413360595703, 
epoch: 166 train_loss: 4.5484819412231445 val_loss: 53.4645881652832, 
epoch: 167 train_loss: 4.523087978363037 val_loss: 53.94586944580078, 
epoch: 168 train_loss: 4.519130229949951 val_loss: 53.347320556640625, 
epoch: 169 train_loss: 4.469418525695801 val_loss: 52.82054138183594, 
epoch: 170 train_loss: 4.482836723327637 val_loss: 52.850914001464844, 
epoch: 171 train_loss: 4.51909065246582 val_loss: 52.878135681152344, 
epoch: 172 train_loss: 4.582395553588867 val_loss: 52.51918029785156, 
Epoch 00174: reducing learning rate of group 0 to 2.5000e-04.
epoch: 173 train_loss: 4.530097484588623 val_loss: 52.83533477783203, 
epoch: 174 train_loss: 4.390981674194336 val_loss: 53.5896110534668, 
epoch: 175 train_loss: 3.938685417175293 val_loss: 53.38353729248047, 
epoch: 176 train_loss: 3.656745195388794 val_loss: 53.56850814819336, 
epoch: 177 train_loss: 3.5643393993377686 val_loss: 53.68464660644531, 
epoch: 178 train_loss: 3.5423429012298584 val_loss: 53.75481033325195, 
epoch: 179 train_loss: 3.5577545166015625 val_loss: 53.611488342285156, 
epoch: 180 train_loss: 3.6154050827026367 val_loss: 53.6109504699707, 
epoch: 181 train_loss: 3.7113935947418213 val_loss: 53.475975036621094, 
epoch: 182 train_loss: 3.8125193119049072 val_loss: 52.96200942993164, 
epoch: 183 train_loss: 3.9347403049468994 val_loss: 52.82487869262695, 
epoch: 184 train_loss: 4.024460315704346 val_loss: 52.55348205566406, 
epoch: 185 train_loss: 4.105624198913574 val_loss: 52.853092193603516, 
epoch: 186 train_loss: 4.15888786315918 val_loss: 52.964027404785156, 
epoch: 187 train_loss: 4.1691083908081055 val_loss: 52.83563995361328, 
epoch: 188 train_loss: 4.13852071762085 val_loss: 52.35979461669922, 
epoch: 189 train_loss: 4.063994884490967 val_loss: 51.83869552612305, 
epoch: 190 train_loss: 3.8603484630584717 val_loss: 51.92223358154297, 
epoch: 191 train_loss: 3.6928398609161377 val_loss: 52.246742248535156, 
epoch: 192 train_loss: 3.5544519424438477 val_loss: 52.29154968261719, 
epoch: 193 train_loss: 3.447540044784546 val_loss: 52.367835998535156, 
epoch: 194 train_loss: 3.385908603668213 val_loss: 52.25251770019531, 
epoch: 195 train_loss: 3.333838701248169 val_loss: 52.06281280517578, 
epoch: 196 train_loss: 3.287564277648926 val_loss: 51.897377014160156, 
epoch: 197 train_loss: 3.2581987380981445 val_loss: 51.65042495727539, 
epoch: 198 train_loss: 3.222808837890625 val_loss: 51.85160827636719, 
epoch: 199 train_loss: 3.2129251956939697 val_loss: 51.44771194458008, save best model
epoch: 200 train_loss: 3.1871414184570312 val_loss: 51.561222076416016, 
epoch: 201 train_loss: 3.174084424972534 val_loss: 51.46749496459961, 
epoch: 202 train_loss: 3.146939992904663 val_loss: 51.54030227661133, 
epoch: 203 train_loss: 3.1491942405700684 val_loss: 51.597900390625, 
epoch: 204 train_loss: 3.1307830810546875 val_loss: 51.62069320678711, 
epoch: 205 train_loss: 3.1174771785736084 val_loss: 51.66999816894531, 
epoch: 206 train_loss: 3.1177093982696533 val_loss: 51.743404388427734, 
epoch: 207 train_loss: 3.1080849170684814 val_loss: 51.529598236083984, 
epoch: 208 train_loss: 3.0882036685943604 val_loss: 51.58390808105469, 
epoch: 209 train_loss: 3.086212158203125 val_loss: 51.41620635986328, save best model
epoch: 210 train_loss: 3.0806703567504883 val_loss: 51.270503997802734, save best model
epoch: 211 train_loss: 3.07373046875 val_loss: 51.36268997192383, 
epoch: 212 train_loss: 3.0833115577697754 val_loss: 51.362064361572266, 
epoch: 213 train_loss: 3.074347734451294 val_loss: 51.30451965332031, 
epoch: 214 train_loss: 3.0892446041107178 val_loss: 51.10521697998047, save best model
epoch: 215 train_loss: 3.08834171295166 val_loss: 51.30155563354492, 
epoch: 216 train_loss: 3.1072897911071777 val_loss: 51.39948654174805, 
epoch: 217 train_loss: 3.1173386573791504 val_loss: 51.43882369995117, 
epoch: 218 train_loss: 3.1484568119049072 val_loss: 51.46821594238281, 
epoch: 219 train_loss: 3.1847569942474365 val_loss: 51.42610549926758, 
epoch: 220 train_loss: 3.222865343093872 val_loss: 51.595497131347656, 
epoch: 221 train_loss: 3.255227565765381 val_loss: 51.896488189697266, 
epoch: 222 train_loss: 3.2758917808532715 val_loss: 52.03168487548828, 
epoch: 223 train_loss: 3.2998147010803223 val_loss: 52.09128189086914, 
epoch: 224 train_loss: 3.3083696365356445 val_loss: 51.94095993041992, 
epoch: 225 train_loss: 3.3075411319732666 val_loss: 51.76577377319336, 
epoch: 226 train_loss: 3.3033406734466553 val_loss: 51.69829177856445, 
epoch: 227 train_loss: 3.3201024532318115 val_loss: 51.666072845458984, 
epoch: 228 train_loss: 3.331319570541382 val_loss: 51.76182556152344, 
epoch: 229 train_loss: 3.342869758605957 val_loss: 52.12150573730469, 
epoch: 230 train_loss: 3.3527398109436035 val_loss: 52.02547836303711, 
epoch: 231 train_loss: 3.328125238418579 val_loss: 52.01638412475586, 
epoch: 232 train_loss: 3.305816888809204 val_loss: 51.868160247802734, 
epoch: 233 train_loss: 3.3295531272888184 val_loss: 51.684017181396484, 
epoch: 234 train_loss: 3.3098809719085693 val_loss: 51.522911071777344, 
epoch: 235 train_loss: 3.283374071121216 val_loss: 51.65016174316406, 
epoch: 236 train_loss: 3.2866950035095215 val_loss: 51.88646697998047, 
epoch: 237 train_loss: 3.302684783935547 val_loss: 52.47867202758789, 
epoch: 238 train_loss: 3.314215898513794 val_loss: 52.7059326171875, 
epoch: 239 train_loss: 3.358569383621216 val_loss: 52.9007682800293, 
epoch: 240 train_loss: 3.3594329357147217 val_loss: 52.7773323059082, 
epoch: 241 train_loss: 3.334559440612793 val_loss: 52.467872619628906, 
epoch: 242 train_loss: 3.3280434608459473 val_loss: 52.401153564453125, 
epoch: 243 train_loss: 3.3474104404449463 val_loss: 52.35051345825195, 
epoch: 244 train_loss: 3.3530023097991943 val_loss: 52.12102508544922, 
epoch: 245 train_loss: 3.3687174320220947 val_loss: 52.0900993347168, 
epoch: 246 train_loss: 3.386112928390503 val_loss: 52.305702209472656, 
epoch: 247 train_loss: 3.4074416160583496 val_loss: 52.10543441772461, 
epoch: 248 train_loss: 3.4298219680786133 val_loss: 52.14177322387695, 
epoch: 249 train_loss: 3.473198890686035 val_loss: 52.12050247192383, 
epoch: 250 train_loss: 3.456275701522827 val_loss: 51.774452209472656, 
epoch: 251 train_loss: 3.469212532043457 val_loss: 51.33362579345703, 
epoch: 252 train_loss: 3.476914167404175 val_loss: 51.37641143798828, 
epoch: 253 train_loss: 3.46136474609375 val_loss: 51.938053131103516, 
epoch: 254 train_loss: 3.4155824184417725 val_loss: 52.73225021362305, 
epoch: 255 train_loss: 3.356649875640869 val_loss: 53.26305389404297, 
epoch: 256 train_loss: 3.3100333213806152 val_loss: 53.377532958984375, 
epoch: 257 train_loss: 3.250253200531006 val_loss: 53.00617218017578, 
epoch: 258 train_loss: 3.19587779045105 val_loss: 52.480552673339844, 
epoch: 259 train_loss: 3.1731276512145996 val_loss: 52.51700973510742, 
epoch: 260 train_loss: 3.1750071048736572 val_loss: 52.07062530517578, 
epoch: 261 train_loss: 3.1551780700683594 val_loss: 51.582176208496094, 
epoch: 262 train_loss: 3.1473488807678223 val_loss: 51.38604736328125, 
epoch: 263 train_loss: 3.1594161987304688 val_loss: 51.72150802612305, 
epoch: 264 train_loss: 3.1606528759002686 val_loss: 52.416160583496094, 
Epoch 00266: reducing learning rate of group 0 to 1.2500e-04.
epoch: 265 train_loss: 3.165801763534546 val_loss: 52.43650817871094, 
epoch: 266 train_loss: 3.2044942378997803 val_loss: 51.664241790771484, 
epoch: 267 train_loss: 2.9933786392211914 val_loss: 51.657466888427734, 
epoch: 268 train_loss: 2.7783303260803223 val_loss: 51.659481048583984, 
epoch: 269 train_loss: 2.6959424018859863 val_loss: 51.61466598510742, 
epoch: 270 train_loss: 2.650587320327759 val_loss: 51.517822265625, 
epoch: 271 train_loss: 2.6248040199279785 val_loss: 51.556400299072266, 
epoch: 272 train_loss: 2.6059131622314453 val_loss: 51.413291931152344, 
epoch: 273 train_loss: 2.5912668704986572 val_loss: 51.377723693847656, 
epoch: 274 train_loss: 2.5801961421966553 val_loss: 51.48158645629883, 
epoch: 275 train_loss: 2.5640177726745605 val_loss: 51.4355583190918, 
epoch: 276 train_loss: 2.5604214668273926 val_loss: 51.347164154052734, 
epoch: 277 train_loss: 2.5705490112304688 val_loss: 51.23952102661133, 
epoch: 278 train_loss: 2.556217670440674 val_loss: 51.13608169555664, 
epoch: 279 train_loss: 2.5618083477020264 val_loss: 51.259944915771484, 
epoch: 280 train_loss: 2.561713457107544 val_loss: 51.23686981201172, 
epoch: 281 train_loss: 2.547745704650879 val_loss: 51.11281967163086, 
epoch: 282 train_loss: 2.5421671867370605 val_loss: 51.10824966430664, 
epoch: 283 train_loss: 2.559542655944824 val_loss: 50.99702072143555, save best model
epoch: 284 train_loss: 2.560544729232788 val_loss: 50.992950439453125, save best model
epoch: 285 train_loss: 2.560532569885254 val_loss: 51.055782318115234, 
epoch: 286 train_loss: 2.564432382583618 val_loss: 51.053123474121094, 
epoch: 287 train_loss: 2.5616586208343506 val_loss: 51.119529724121094, 
epoch: 288 train_loss: 2.5606956481933594 val_loss: 51.0837287902832, 
epoch: 289 train_loss: 2.5792105197906494 val_loss: 51.120975494384766, 
epoch: 290 train_loss: 2.57958984375 val_loss: 51.056461334228516, 
epoch: 291 train_loss: 2.5725455284118652 val_loss: 51.120670318603516, 
epoch: 292 train_loss: 2.571713447570801 val_loss: 51.02784729003906, 
epoch: 293 train_loss: 2.570816993713379 val_loss: 50.93891906738281, save best model
epoch: 294 train_loss: 2.562077045440674 val_loss: 51.06155776977539, 
epoch: 295 train_loss: 2.5639333724975586 val_loss: 50.91230010986328, save best model
epoch: 296 train_loss: 2.544602870941162 val_loss: 50.82219314575195, save best model
epoch: 297 train_loss: 2.538904905319214 val_loss: 50.81700897216797, save best model
epoch: 298 train_loss: 2.5301859378814697 val_loss: 50.75187683105469, save best model
epoch: 299 train_loss: 2.5120866298675537 val_loss: 50.7049674987793, save best model
epoch: 300 train_loss: 2.503089189529419 val_loss: 50.67182922363281, save best model
epoch: 301 train_loss: 2.5051674842834473 val_loss: 50.72945785522461, 
epoch: 302 train_loss: 2.4970133304595947 val_loss: 50.71769714355469, 
epoch: 303 train_loss: 2.4880263805389404 val_loss: 50.75027847290039, 
epoch: 304 train_loss: 2.4787073135375977 val_loss: 50.54397964477539, save best model
epoch: 305 train_loss: 2.483485698699951 val_loss: 50.630741119384766, 
epoch: 306 train_loss: 2.4931886196136475 val_loss: 50.521087646484375, save best model
epoch: 307 train_loss: 2.4894487857818604 val_loss: 50.61293411254883, 
epoch: 308 train_loss: 2.500627279281616 val_loss: 50.671104431152344, 
epoch: 309 train_loss: 2.4935553073883057 val_loss: 50.72239303588867, 
epoch: 310 train_loss: 2.5146877765655518 val_loss: 50.65143966674805, 
epoch: 311 train_loss: 2.5047483444213867 val_loss: 50.86146926879883, 
epoch: 312 train_loss: 2.5253517627716064 val_loss: 50.78932571411133, 
epoch: 313 train_loss: 2.533801317214966 val_loss: 50.87899398803711, 
epoch: 314 train_loss: 2.542941093444824 val_loss: 50.88208770751953, 
epoch: 315 train_loss: 2.541398763656616 val_loss: 50.84260177612305, 
epoch: 316 train_loss: 2.5437333583831787 val_loss: 50.828887939453125, 
epoch: 317 train_loss: 2.5618231296539307 val_loss: 50.86348342895508, 
epoch: 318 train_loss: 2.548055410385132 val_loss: 50.808773040771484, 
epoch: 319 train_loss: 2.5644936561584473 val_loss: 50.696144104003906, 
epoch: 320 train_loss: 2.547046661376953 val_loss: 50.80440139770508, 
epoch: 321 train_loss: 2.534651517868042 val_loss: 50.904258728027344, 
epoch: 322 train_loss: 2.5168983936309814 val_loss: 50.671329498291016, 
epoch: 323 train_loss: 2.506690740585327 val_loss: 50.70451354980469, 
epoch: 324 train_loss: 2.492849111557007 val_loss: 50.65983200073242, 
epoch: 325 train_loss: 2.503291368484497 val_loss: 50.621585845947266, 
epoch: 326 train_loss: 2.4952757358551025 val_loss: 50.46213912963867, save best model
epoch: 327 train_loss: 2.4837071895599365 val_loss: 50.54911422729492, 
epoch: 328 train_loss: 2.4751482009887695 val_loss: 50.473907470703125, 
epoch: 329 train_loss: 2.4912984371185303 val_loss: 50.53450393676758, 
epoch: 330 train_loss: 2.495907783508301 val_loss: 50.44574737548828, save best model
epoch: 331 train_loss: 2.5017635822296143 val_loss: 50.40946578979492, save best model
epoch: 332 train_loss: 2.5081217288970947 val_loss: 50.41907501220703, 
epoch: 333 train_loss: 2.519596815109253 val_loss: 50.30448532104492, save best model
epoch: 334 train_loss: 2.5103042125701904 val_loss: 50.4191780090332, 
epoch: 335 train_loss: 2.5216164588928223 val_loss: 50.53791809082031, 
epoch: 336 train_loss: 2.529236078262329 val_loss: 50.80278015136719, 
epoch: 337 train_loss: 2.5484399795532227 val_loss: 50.99470901489258, 
epoch: 338 train_loss: 2.5518860816955566 val_loss: 51.20082092285156, 
epoch: 339 train_loss: 2.5449869632720947 val_loss: 51.21600341796875, 
epoch: 340 train_loss: 2.559255599975586 val_loss: 51.382572174072266, 
epoch: 341 train_loss: 2.564664602279663 val_loss: 51.25647735595703, 
epoch: 342 train_loss: 2.5583934783935547 val_loss: 51.09260559082031, 
epoch: 343 train_loss: 2.5420727729797363 val_loss: 51.089508056640625, 
epoch: 344 train_loss: 2.5116474628448486 val_loss: 50.86858367919922, 
epoch: 345 train_loss: 2.5043160915374756 val_loss: 50.70845031738281, 
epoch: 346 train_loss: 2.4822824001312256 val_loss: 50.686683654785156, 
epoch: 347 train_loss: 2.4635257720947266 val_loss: 50.740966796875, 
epoch: 348 train_loss: 2.4561967849731445 val_loss: 50.65993118286133, 
epoch: 349 train_loss: 2.449313163757324 val_loss: 50.638572692871094, 
epoch: 350 train_loss: 2.443648099899292 val_loss: 50.59394073486328, 
epoch: 351 train_loss: 2.456404685974121 val_loss: 50.485023498535156, 
epoch: 352 train_loss: 2.447164535522461 val_loss: 50.49781799316406, 
epoch: 353 train_loss: 2.4473161697387695 val_loss: 50.45262145996094, 
epoch: 354 train_loss: 2.4387266635894775 val_loss: 50.26946258544922, save best model
epoch: 355 train_loss: 2.4386825561523438 val_loss: 50.332862854003906, 
epoch: 356 train_loss: 2.4386911392211914 val_loss: 50.238014221191406, save best model
epoch: 357 train_loss: 2.4269514083862305 val_loss: 50.462520599365234, 
epoch: 358 train_loss: 2.4175310134887695 val_loss: 50.435054779052734, 
epoch: 359 train_loss: 2.4235541820526123 val_loss: 50.4657096862793, 
epoch: 360 train_loss: 2.4282915592193604 val_loss: 50.364688873291016, 
epoch: 361 train_loss: 2.4179747104644775 val_loss: 50.533203125, 
epoch: 362 train_loss: 2.43066668510437 val_loss: 50.695831298828125, 
epoch: 363 train_loss: 2.4262359142303467 val_loss: 50.76825714111328, 
epoch: 364 train_loss: 2.407163381576538 val_loss: 51.09170913696289, 
epoch: 365 train_loss: 2.393768072128296 val_loss: 50.871707916259766, 
epoch: 366 train_loss: 2.377295970916748 val_loss: 50.80704116821289, 
epoch: 367 train_loss: 2.3723392486572266 val_loss: 50.5956916809082, 
epoch: 368 train_loss: 2.3635523319244385 val_loss: 50.4666748046875, 
epoch: 369 train_loss: 2.3761916160583496 val_loss: 50.4284553527832, 
epoch: 370 train_loss: 2.36675763130188 val_loss: 50.32964324951172, 
epoch: 371 train_loss: 2.3634493350982666 val_loss: 50.261329650878906, 
epoch: 372 train_loss: 2.3609354496002197 val_loss: 50.31129837036133, 
epoch: 373 train_loss: 2.3800973892211914 val_loss: 50.24373245239258, 
epoch: 374 train_loss: 2.3691625595092773 val_loss: 50.40354919433594, 
epoch: 375 train_loss: 2.3654420375823975 val_loss: 50.160274505615234, save best model
epoch: 376 train_loss: 2.3685765266418457 val_loss: 50.233726501464844, 
epoch: 377 train_loss: 2.392025947570801 val_loss: 50.281517028808594, 
epoch: 378 train_loss: 2.384178400039673 val_loss: 50.27638244628906, 
epoch: 379 train_loss: 2.3968660831451416 val_loss: 50.38329315185547, 
epoch: 380 train_loss: 2.3899292945861816 val_loss: 50.38946533203125, 
epoch: 381 train_loss: 2.405088186264038 val_loss: 50.563297271728516, 
epoch: 382 train_loss: 2.389831781387329 val_loss: 50.58686828613281, 
epoch: 383 train_loss: 2.3884267807006836 val_loss: 50.7245979309082, 
epoch: 384 train_loss: 2.3941164016723633 val_loss: 50.81856155395508, 
epoch: 385 train_loss: 2.391840934753418 val_loss: 50.888492584228516, 
epoch: 386 train_loss: 2.396637439727783 val_loss: 50.97663116455078, 
epoch: 387 train_loss: 2.4129798412323 val_loss: 50.76209259033203, 
epoch: 388 train_loss: 2.4101202487945557 val_loss: 50.54458236694336, 
epoch: 389 train_loss: 2.4128260612487793 val_loss: 50.404598236083984, 
epoch: 390 train_loss: 2.422506332397461 val_loss: 50.4842414855957, 
epoch: 391 train_loss: 2.42572021484375 val_loss: 50.42741012573242, 
epoch: 392 train_loss: 2.4307005405426025 val_loss: 50.55771255493164, 
epoch: 393 train_loss: 2.4374618530273438 val_loss: 50.620361328125, 
epoch: 394 train_loss: 2.4451777935028076 val_loss: 50.56953811645508, 
epoch: 395 train_loss: 2.469611883163452 val_loss: 50.61319351196289, 
epoch: 396 train_loss: 2.479707956314087 val_loss: 50.589866638183594, 
epoch: 397 train_loss: 2.477078914642334 val_loss: 50.67987060546875, 
epoch: 398 train_loss: 2.4874463081359863 val_loss: 50.75717544555664, 
epoch: 399 train_loss: 2.4811947345733643 val_loss: 50.900001525878906, 
epoch: 400 train_loss: 2.4959046840667725 val_loss: 50.90434265136719, 
epoch: 401 train_loss: 2.429766893386841 val_loss: 50.373046875, 
epoch: 402 train_loss: 2.4031896591186523 val_loss: 50.347232818603516, 
epoch: 403 train_loss: 2.404794931411743 val_loss: 50.32680892944336, 
epoch: 404 train_loss: 2.4086132049560547 val_loss: 50.45787811279297, 
epoch: 405 train_loss: 2.4214587211608887 val_loss: 50.50831604003906, 
epoch: 406 train_loss: 2.416809558868408 val_loss: 50.61191940307617, 
epoch: 407 train_loss: 2.4152681827545166 val_loss: 50.718379974365234, 
epoch: 408 train_loss: 2.386766195297241 val_loss: 50.62180709838867, 
epoch: 409 train_loss: 2.389449119567871 val_loss: 50.847660064697266, 
epoch: 410 train_loss: 2.382678985595703 val_loss: 50.77824783325195, 
epoch: 411 train_loss: 2.3667800426483154 val_loss: 50.69805908203125, 
epoch: 412 train_loss: 2.3429670333862305 val_loss: 50.671607971191406, 
epoch: 413 train_loss: 2.3520658016204834 val_loss: 50.61078643798828, 
epoch: 414 train_loss: 2.3466508388519287 val_loss: 50.46686553955078, 
epoch: 415 train_loss: 2.3189427852630615 val_loss: 50.419403076171875, 
epoch: 416 train_loss: 2.3238279819488525 val_loss: 50.4122200012207, 
epoch: 417 train_loss: 2.3076469898223877 val_loss: 50.16889953613281, 
epoch: 418 train_loss: 2.3033504486083984 val_loss: 50.207611083984375, 
epoch: 419 train_loss: 2.3000454902648926 val_loss: 50.25135040283203, 
epoch: 420 train_loss: 2.3062493801116943 val_loss: 50.21807861328125, 
epoch: 421 train_loss: 2.289168357849121 val_loss: 50.196407318115234, 
epoch: 422 train_loss: 2.292119264602661 val_loss: 50.15238952636719, save best model
epoch: 423 train_loss: 2.3016273975372314 val_loss: 50.45420837402344, 
epoch: 424 train_loss: 2.3106870651245117 val_loss: 50.287635803222656, 
epoch: 425 train_loss: 2.3137118816375732 val_loss: 50.32868957519531, 
epoch: 426 train_loss: 2.3359503746032715 val_loss: 50.36179733276367, 
epoch: 427 train_loss: 2.330526113510132 val_loss: 50.6297607421875, 
epoch: 428 train_loss: 2.3389291763305664 val_loss: 50.62660598754883, 
epoch: 429 train_loss: 2.3480634689331055 val_loss: 51.00015640258789, 
epoch: 430 train_loss: 2.3465683460235596 val_loss: 50.91231155395508, 
epoch: 431 train_loss: 2.350335121154785 val_loss: 50.83375930786133, 
epoch: 432 train_loss: 2.3662803173065186 val_loss: 50.749794006347656, 
epoch: 433 train_loss: 2.3648576736450195 val_loss: 50.6519660949707, 
epoch: 434 train_loss: 2.3748037815093994 val_loss: 50.670467376708984, 
epoch: 435 train_loss: 2.3839597702026367 val_loss: 50.656150817871094, 
epoch: 436 train_loss: 2.4072227478027344 val_loss: 50.38555145263672, 
epoch: 437 train_loss: 2.3960726261138916 val_loss: 50.37882995605469, 
epoch: 438 train_loss: 2.3922221660614014 val_loss: 50.355735778808594, 
epoch: 439 train_loss: 2.394394874572754 val_loss: 50.22013473510742, 
epoch: 440 train_loss: 2.3734872341156006 val_loss: 50.2795295715332, 
epoch: 441 train_loss: 2.349881649017334 val_loss: 50.16781234741211, 
epoch: 442 train_loss: 2.339430093765259 val_loss: 50.27096939086914, 
epoch: 443 train_loss: 2.3235208988189697 val_loss: 50.25381851196289, 
epoch: 444 train_loss: 2.312551736831665 val_loss: 50.34257888793945, 
epoch: 445 train_loss: 2.325194835662842 val_loss: 50.52882766723633, 
epoch: 446 train_loss: 2.296769142150879 val_loss: 50.457950592041016, 
epoch: 447 train_loss: 2.2918386459350586 val_loss: 50.49407196044922, 
epoch: 448 train_loss: 2.2726316452026367 val_loss: 50.477989196777344, 
epoch: 449 train_loss: 2.2675554752349854 val_loss: 50.5310173034668, 
epoch: 450 train_loss: 2.253781318664551 val_loss: 50.564491271972656, 
epoch: 451 train_loss: 2.2463953495025635 val_loss: 50.48699951171875, 
epoch: 452 train_loss: 2.2486531734466553 val_loss: 50.3965950012207, 
epoch: 453 train_loss: 2.2466142177581787 val_loss: 50.31671905517578, 
epoch: 454 train_loss: 2.2570624351501465 val_loss: 50.397216796875, 
epoch: 455 train_loss: 2.2660365104675293 val_loss: 50.14506530761719, save best model
epoch: 456 train_loss: 2.265599250793457 val_loss: 50.16463088989258, 
epoch: 457 train_loss: 2.2508678436279297 val_loss: 50.24675750732422, 
epoch: 458 train_loss: 2.2451207637786865 val_loss: 50.12724304199219, save best model
epoch: 459 train_loss: 2.2520604133605957 val_loss: 50.15839385986328, 
epoch: 460 train_loss: 2.246975898742676 val_loss: 50.23765563964844, 
epoch: 461 train_loss: 2.2527928352355957 val_loss: 50.28408432006836, 
epoch: 462 train_loss: 2.2581822872161865 val_loss: 50.26219940185547, 
epoch: 463 train_loss: 2.262489080429077 val_loss: 50.381797790527344, 
epoch: 464 train_loss: 2.2441608905792236 val_loss: 50.50100326538086, 
epoch: 465 train_loss: 2.2523975372314453 val_loss: 50.443115234375, 
epoch: 466 train_loss: 2.2450296878814697 val_loss: 50.31495666503906, 
epoch: 467 train_loss: 2.2489397525787354 val_loss: 50.5197868347168, 
epoch: 468 train_loss: 2.246467351913452 val_loss: 50.43032455444336, 
epoch: 469 train_loss: 2.239985227584839 val_loss: 50.24968338012695, 
epoch: 470 train_loss: 2.2489280700683594 val_loss: 50.253108978271484, 
epoch: 471 train_loss: 2.2514772415161133 val_loss: 50.13536834716797, 
epoch: 472 train_loss: 2.2452166080474854 val_loss: 50.149593353271484, 
epoch: 473 train_loss: 2.236267328262329 val_loss: 50.0604248046875, save best model
epoch: 474 train_loss: 2.2365152835845947 val_loss: 50.101165771484375, 
epoch: 475 train_loss: 2.2400434017181396 val_loss: 50.125579833984375, 
epoch: 476 train_loss: 2.2424142360687256 val_loss: 50.091217041015625, 
epoch: 477 train_loss: 2.261942148208618 val_loss: 50.08676528930664, 
epoch: 478 train_loss: 2.2699027061462402 val_loss: 50.00266647338867, save best model
epoch: 479 train_loss: 2.2697982788085938 val_loss: 50.14712142944336, 
epoch: 480 train_loss: 2.29555344581604 val_loss: 50.14475631713867, 
epoch: 481 train_loss: 2.303048849105835 val_loss: 50.174591064453125, 
epoch: 482 train_loss: 2.3064959049224854 val_loss: 50.235023498535156, 
epoch: 483 train_loss: 2.299807548522949 val_loss: 50.24113845825195, 
epoch: 484 train_loss: 2.315549373626709 val_loss: 50.45694351196289, 
epoch: 485 train_loss: 2.297348737716675 val_loss: 50.67308044433594, 
epoch: 486 train_loss: 2.294036865234375 val_loss: 50.94795608520508, 
epoch: 487 train_loss: 2.27394700050354 val_loss: 50.88798141479492, 
epoch: 488 train_loss: 2.2774360179901123 val_loss: 50.85139465332031, 
epoch: 489 train_loss: 2.2624576091766357 val_loss: 50.69593811035156, 
epoch: 490 train_loss: 2.2518410682678223 val_loss: 50.47120666503906, 
epoch: 491 train_loss: 2.2359044551849365 val_loss: 50.4494743347168, 
epoch: 492 train_loss: 2.2292299270629883 val_loss: 50.400882720947266, 
epoch: 493 train_loss: 2.2094268798828125 val_loss: 50.42276382446289, 
epoch: 494 train_loss: 2.1949825286865234 val_loss: 50.355770111083984, 
epoch: 495 train_loss: 2.2034945487976074 val_loss: 50.34286880493164, 
epoch: 496 train_loss: 2.2043871879577637 val_loss: 50.31974792480469, 
epoch: 497 train_loss: 2.22487735748291 val_loss: 50.144195556640625, 
epoch: 498 train_loss: 2.2356443405151367 val_loss: 50.38938522338867, 
epoch: 499 train_loss: 2.25026535987854 val_loss: 50.382232666015625, 
test_loss:  51.8947
0.8186657473500882
